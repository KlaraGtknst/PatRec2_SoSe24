{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1HKXRBrqRTF"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsynIyoUqRTe"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.optimize import minimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etyZTNEfqRTn"
      },
      "source": [
        "def get_simple():\n",
        "    X = np.linspace(-3, 3, 11)\n",
        "    y = np.sin(X)\n",
        "    y+=np.random.randn(11)*.2\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzVBtWY1qRTt"
      },
      "source": [
        "### **1. Training of SVRs via Constrained Optimization** <a class=\"anchor\" id=\"optim\"></a>\n",
        "\n",
        "Throughout this notebook, we assume $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ as $N \\times D$ matrix of training examples and $\\mathbf{t} \\in \\mathbb{R}^N$ as $N$-dimensional vector of training targets.\n",
        "To express the dual SVR in standard form, we express the kernel matrix $K \\in \\mathbb{R}^{NxN}$ such that each entry is $K_{ij} = k(\\mathbf{x}_i , \\mathbf{x}_j)$.\n",
        "\n",
        "The dual form of the SVR was introduced as:\n",
        "  \\begin{align*}\n",
        "  \\widetilde{L}(\\mathbf a,\\widehat{\\mathbf a}) =& - \\frac{1}{2}  \\sum_{n=1}^N  \\sum_{m=1}^N (a_n - \\widehat a _n) (a_m - \\widehat a _m)k(\\mathbf x_n,\\mathbf x_m)\\\\ &- \\epsilon  \\sum_{n=1}^N (a_n + \\widehat a _n) +  \\sum_{m=1}^N (a_n - \\widehat a _n) t_n\n",
        "  \\end{align*}\n",
        "\n",
        "\n",
        "> To simplify the mathematical procedure, transform it first into matrix multiplication form!\n",
        "\n",
        "The optimization objective is given by:\n",
        "\\begin{align}\n",
        "\\max_{\\boldsymbol{a}}\\widetilde{L}(\\mathbf a,\\widehat{\\mathbf a})\n",
        "\\end{align}\n",
        "subject to\n",
        "\\begin{eqnarray*}\n",
        "  0 \\leqslant a_n \\leqslant C\\\\\n",
        "  0 \\leqslant \\widehat a_n \\leqslant C\n",
        "\\end{eqnarray*}\n",
        "\n",
        "Once, we have found the optimum $\\boldsymbol{a}$, the prediction function of the SVR is given by\n",
        "\\begin{equation}\n",
        "y(\\mathbf x) = \\sum_{n=1}^N (a_n- \\widehat a _n)k (\\mathbf x, \\mathbf x _n) +b\n",
        "\\end{equation}\n",
        "where $b \\in \\mathbb{R}$ is the bias parameter.\n",
        "\n",
        "We can estimate $b$ by considering a data point for which $0 < a_n < C$, which must have $\\xi_n = 0$. Therefore this point must satisfy $\\epsilon + y_n - t_n = 0$.\n",
        "\\begin{equation}\n",
        "b = \\frac{1}{N_\\mathcal{M}} \\sum_{n \\in \\mathcal{M}} \\left( t_n - \\epsilon - \\sum_{m \\in \\mathcal{S}} (a_m- \\widehat a _m)k (\\mathbf x_n, \\mathbf x _m)\\right).\n",
        "\\end{equation}\n",
        "Analogous results can be obtained by considering a point for which $0 < \\widehat a_n < C$. $\\mathcal{S} \\subseteq \\{1, \\dots, N\\}$ denotes the set of support vectors and $\\mathcal{M} \\subseteq \\{1, \\dots, N\\}$ denotes the set of support vectors lying\n",
        "on the margin with $N_\\mathcal{M} = |\\mathcal{M}|$.\n",
        "\n",
        "> Below, implement a SVR for a simple regression problem by solving the dual problem above.\n",
        "> For optimization make use of `scipy` and its [Optimization Module](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#sequential-least-squares-programming-slsqp-algorithm-method-slsqp)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLMvftp6qRTy"
      },
      "source": [
        "class RBFKernel:\n",
        "    def __init__(self, gamma=1):\n",
        "        \"\"\"Computes RBF kernel matrix between X_1 and X_2.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Hyperparameter of RBF kernel.\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def __call__(self, X_1, X_2):\n",
        "        \"\"\"Computes the kernel matrix.\n",
        "\n",
        "        Args:\n",
        "            X_1 (array-like): Input samples in shape (N, D).\n",
        "            X_2 (array-like): Input samples in shape (N, D).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Kernel matrix of shape shape (N, M)\n",
        "        \"\"\"\n",
        "        # Transform input to numpy arrays.\n",
        "        X_1 = np.array(X_1).reshape(-1, 1)\n",
        "        X_2 = np.array(X_2).reshape(-1, 1)\n",
        "\n",
        "        # Compute NxM Euclidean distance matrix.\n",
        "        E = np.sqrt((np.square(X_1[:,np.newaxis]-X_2).sum(axis=2))) # <-- SOLUTION\n",
        "\n",
        "        # Compute NxM kernel matrix based on Euclidean distances.\n",
        "        K = np.exp(-self.gamma * E**2) # <-- SOLUTION\n",
        "        return K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHdSlFamqRT3"
      },
      "source": [
        "class SVR:\n",
        "    def __init__(self, kernel_func, eps=0.2, C=1.0, random_state=42):\n",
        "        \"\"\"Implementation of a C-SVM for regression.\n",
        "        Args:\n",
        "            C (float): Regularization parameter. The strength of the regularization is inversely\n",
        "                proportional to C. Must be strictly positive. (default=1.0)\n",
        "            eps (float): ...\n",
        "            kernel_func (callable): Specifies the kernel type to be used in the algorithm.\n",
        "            random_state (int): Random state to ensure reproducibility when initializing  a values.\n",
        "        \"\"\"\n",
        "        self.C = C\n",
        "        self.eps = eps\n",
        "        self.kernel_func = kernel_func\n",
        "        self.random_state = random_state\n",
        "\n",
        "\n",
        "    def fit(self, X, t):\n",
        "        \"\"\"Fit the SVM model according to the given training data.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Training samples of shape (N, D).\n",
        "            t (array-like): Training targets of shape (N).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted SVM object.\n",
        "        \"\"\"\n",
        "        # Transform to ndarray.\n",
        "        X, t = np.array(X), np.array(t)\n",
        "        # Compute NxN kernel matrix based on kernel_func.\n",
        "        K = self.kernel_func(X, X)\n",
        "        # Number of samples.\n",
        "        N = len(X)\n",
        "\n",
        "        # Optimization\n",
        "        # Step 1: Define the loss function and its gradient.\n",
        "        def loss(a):\n",
        "            # Compute loss for given a.\n",
        "            a, a_hat = a[:N], a[N:]\n",
        "            loss = - (\n",
        "                - .5*( a@K@a - 2*a@K@a_hat + a_hat@K@a_hat)\n",
        "                - self.eps * np.sum(a + a_hat)\n",
        "                + (a - a_hat) @ t\n",
        "            )\n",
        "            return loss\n",
        "\n",
        "        def jac(a):\n",
        "            # Compute gradient of loss function w.r.t. a.\n",
        "            a, a_hat = a[:N], a[N:]\n",
        "            grad_a = -.5*(2*a@K  - 2*K@a_hat) - self.eps +  t\n",
        "            grad_a_hat = -.5*(2*a_hat@K  - 2*a@K) - self.eps - t\n",
        "            grad = -np.concatenate((grad_a, grad_a_hat), axis=0)\n",
        "            return grad\n",
        "\n",
        "        # Step 2: Define the Constraints.\n",
        "        # We need to write the contraints in matrix notation:\n",
        "        # - for inequalities: Ax <= b\n",
        "        # - for eqalities cx = d\n",
        "        # Note that x = a in our example.\n",
        "        # 'fun' in the constraints needs to be adapted such that\n",
        "        # 0 <= lambda a: ....\n",
        "\n",
        "        # Set up the constraints:\n",
        "        # Example: {'type': 'eq', 'fun': lambda a: a**2, 'jac': lambda a: 2*a}\n",
        "        A = np.vstack((-np.eye(N*2), np.eye(N*2)))\n",
        "        b = np.concatenate((np.zeros(N*2), self.C * np.ones(N*2)))\n",
        "        constraints = (\n",
        "            {'type': 'ineq', 'fun': lambda a: b - np.dot(A, a), 'jac': lambda a: -A},\n",
        "        )\n",
        "\n",
        "        # Optimize the a vector.\n",
        "        a0 = np.random.RandomState(self.random_state).rand(2*N)  # Set an initial a vector.\n",
        "        self.a_ = minimize(loss, a0, jac=jac, constraints=constraints, method='SLSQP').x\n",
        "        self.a_[np.isclose(self.a_, 0)] = 0  # zero out nearly zeros\n",
        "        self.a_[np.isclose(self.a_, self.C)] = self.C  # round the ones that are nearly C\n",
        "\n",
        "\n",
        "        # Determine indices of support vectors.\n",
        "        self.a_, self.a_hat = self.a_[:N], self.a_[N:]\n",
        "        idx_support = self.a_ > 0\n",
        "        idx_hat_support = self.a_hat > 0\n",
        "\n",
        "        # Determine indices of support vectors that lie on the margin.\n",
        "        idx_margin = (0 < self.a_) & (self.a_ < self.C)\n",
        "        idx_hat_margin = (0 < self.a_hat) & (self.a_hat < self.C)\n",
        "\n",
        "        # Determine bias parameter.\n",
        "        a_diff = self.a_ - self.a_hat\n",
        "        if np.sum(idx_margin) > 0:\n",
        "            self.b_ = np.sum(t[idx_margin] - self.eps - K[idx_margin]@a_diff) / np.sum(idx_margin)\n",
        "        elif np.sum(idx_hat_margin) > 0:\n",
        "            self.b_ = np.sum(-t[idx_hat_margin] + self.eps - K[idx_hat_margin]@a_diff) / np.sum(idx_hat_margin)\n",
        "\n",
        "        # Store support vectors including their targets and a.\n",
        "        self.X_support_ = X[idx_support | idx_hat_support]\n",
        "        self.t_support_ = t[idx_support | idx_hat_support]\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Perform regression on samples in X.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Input samples whose targets are to be predicted.\n",
        "\n",
        "        Returns:\n",
        "            y (array-like): Predicted target of samples in X.\n",
        "        \"\"\"\n",
        "        K = self.kernel_func(X, self.X_support_)\n",
        "        idx = self.a_ + self.a_hat > 0\n",
        "        d = K @ (self.a_[idx]-self.a_hat[idx]) + self.b_\n",
        "        return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WAIEcoxqRT6"
      },
      "source": [
        "> Train the SVR on the given dataset and plot its support vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkBv0YN9qRT8",
        "outputId": "1d4965f2-8704-45eb-a573-4a0948f6eb40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "X, y = get_simple()\n",
        "\n",
        "kernel_func = RBFKernel(gamma=1)\n",
        "# Create SVR with C being infinity and a linear kernel.\n",
        "\n",
        "eps = .2\n",
        "C = 1\n",
        "svm = SVR(kernel_func=kernel_func, C=C , eps=eps).fit(X=X, t=y) # <-- SOLUTION\n",
        "\n",
        "# Make predictions for the SVM on X.\n",
        "axis = np.linspace(-3, 3, 201)\n",
        "y_pred = svm.predict(axis) # <-- SOLUTION\n",
        "\n",
        "# Visualize predictions of the SVM.\n",
        "fig, ax = plt.subplots()\n",
        "plt.scatter(X, y)\n",
        "plt.plot(axis, y_pred, zorder=5)\n",
        "plt.fill_between(axis, y_pred-eps, y_pred+eps, alpha=.3, color='r')\n",
        "plt.scatter(svm.X_support_, svm.t_support_, color='g', s=100, facecolors='none', edgecolors='g', label='support vectors')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_simple' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5f72217e9e61>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkernel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRBFKernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create SVR with C being infinity and a linear kernel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_simple' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QtiM15byI5Yo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}