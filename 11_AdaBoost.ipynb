{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vqwZQO2fF4z"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMDEtS6DfF40"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "\n",
        "from sklearn.datasets import make_circles, load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thOT0Yq_fF41"
      },
      "source": [
        "# **AdaBoost**\n",
        "\n",
        "Boosting is a powerful technique used to construct a committee whose performance is significantly better than that of every single model within that committee. It delivers the most significant improvements when the predictions of individual models are slightly better than random predictions. In this case, the individual models are also called weak learners.\n",
        "\n",
        "It consists of the following steps:\n",
        "- The individual models, such as base classifiers are trained in sequence.\n",
        "- Each base classifier is trained using weighted training data.\n",
        "- The weighting coefficients of each sample depend on the performance of the previously trained base classifiers. In particular, misclassified data points receive a higher weight for the next classifier's training.\n",
        "- After training all the base classifiers, these are combined through a suitably weighted average of individual model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9hOCQX4fF41"
      },
      "outputs": [],
      "source": [
        "np.random.seed(2)\n",
        "X, y = make_circles(factor=.2, noise=.2)\n",
        "y[y==0] = -1\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9nGO5dQfF42"
      },
      "outputs": [],
      "source": [
        "def plot_contour(clf, X, y, sample_weight=None):\n",
        "    xx, yy = np.mgrid[-3:3:.1,-3:3:.1]\n",
        "    zz = np.c_[xx.ravel(), yy.ravel()]\n",
        "    proba = clf.predict(zz).reshape(xx.shape)\n",
        "\n",
        "    # Normalize in intervall 40 to 240\n",
        "    if sample_weight is not None:\n",
        "        sample_weight = np.clip(sample_weight, 20, 500)\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=2, s=sample_weight, cmap='coolwarm', edgecolors='k')\n",
        "    plt.contourf(xx, yy, proba, zorder=1, alpha=.5, cmap='coolwarm')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oamUZObMfF42"
      },
      "source": [
        "> Implement the AdaBoost algorithm.\n",
        "\n",
        "Use the `DecisionTreeClassifier(max_depth=1)` as the weak learner. Its method `.fit(X, y, sample_weight=sample_weight)` has a additional argument such that we can specify the sample weights considered in the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKMTU7KnfF42"
      },
      "outputs": [],
      "source": [
        "class Boosting:\n",
        "    def __init__(self, n_models=100):\n",
        "        self.n_models = n_models\n",
        "\n",
        "    def fit(self, X, y, plot=False):\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "    def predict(self, X):\n",
        "####################\n",
        "# Your Code Here   #\n",
        "####################\n",
        "\n",
        "clf = Boosting(n_models=20)\n",
        "clf.fit(X, y, plot=False)\n",
        "plot_contour(clf, X, y, clf.sample_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sI6RUmtfF42"
      },
      "source": [
        "> Visualize the decision boundaries for different number of estimators during training similar to Figure 14.2 in [Bishop2006]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk-KHM7zfF42"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxyZQKT1fF42"
      },
      "source": [
        "> Test your implementation using the given more difficult dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXce7UkufF42"
      },
      "outputs": [],
      "source": [
        "X, y = load_digits(return_X_y=True)\n",
        "idx = (y==1) | (y == 7)\n",
        "X, y = X[idx], y[idx]\n",
        "y[y==1] = 1\n",
        "y[y==7] = -1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,  random_state=42)\n",
        "\n",
        "plt.imshow(X_train[np.random.randint(len(X_train))].reshape(8, 8))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlAbb2zafF43"
      },
      "source": [
        "> Train multiple Boosting models with an increasing number of estimators.  In each iteration, store training error, test error, alphas and epsilons. Investigate how these quantities behave with respect to the estimators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkli7NnjfF43"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNNIKJ1gfF43"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHDvkjrofF43"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzf7t9cEfF43"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Your Code Here   #\n",
        "####################"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}