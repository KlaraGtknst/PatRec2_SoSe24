{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd9-Zd2cfDg6"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "S-aJC4B8fDg7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "s421zyyPfDg7"
      },
      "outputs": [],
      "source": [
        "def get_simple():\n",
        "    X = np.linspace(-3, 3, 11)\n",
        "    y = np.sin(X)\n",
        "    y+=np.random.randn(11)*.2\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3NPUjZcfDg8"
      },
      "source": [
        "### **1. Training of SVRs via Constrained Optimization** <a class=\"anchor\" id=\"optim\"></a>\n",
        "\n",
        "Throughout this notebook, we assume $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ as $N \\times D$ matrix of training examples and $\\mathbf{t} \\in \\mathbb{R}^N$ as $N$-dimensional vector of training targets.\n",
        "To express the dual SVR in standard form, we express the kernel matrix $K \\in \\mathbb{R}^{NxN}$ such that each entry is $K_{ij} = k(\\mathbf{x}_i , \\mathbf{x}_j)$.\n",
        "\n",
        "The dual form of the SVR was introduced as:\n",
        "\n",
        "  \\begin{align*}\n",
        "  \\widetilde{L}(\\mathbf a,\\widehat{\\mathbf a}) =& - \\frac{1}{2}  \\sum_{n=1}^N  \\sum_{m=1}^N (a_n - \\widehat a _n) (a_m - \\widehat a _m)k(\\mathbf x_n,\\mathbf x_m)\\\\ &- \\epsilon  \\sum_{n=1}^N (a_n + \\widehat a _n) +  \\sum_{m=1}^N (a_n - \\widehat a _n) t_n\n",
        "  \\end{align*}\n",
        "\n",
        "\n",
        "> To simplify the mathematical procedure, transform it first into matrix multiplication form!\n",
        "\n",
        "  \\begin{align*}\n",
        "  \\widetilde{L}(\\mathbf a,\\widehat{\\mathbf a}) =& - \\frac{1}{2}  (\\mathbf a - \\widehat{\\mathbf a})^T \\mathbf K (\\mathbf a - \\widehat{\\mathbf a})\\\\ &- \\epsilon  (\\mathbf a + \\widehat{\\mathbf a})^T * \\mathbf {1} +   (\\mathbf a - \\widehat{\\mathbf a})^T \\mathbf t\n",
        "  \\end{align*}\n",
        "\n",
        "The optimization objective is given by:\n",
        "\\begin{align}\n",
        "\\max_{\\boldsymbol{a}}\\widetilde{L}(\\mathbf a,\\widehat{\\mathbf a})\n",
        "\\end{align}\n",
        "subject to\n",
        "\\begin{align}\n",
        "  0 \\leqslant a_n \\leqslant C\\\\\n",
        "  0 \\leqslant \\widehat a_n \\leqslant C\n",
        "\\end{align}\n",
        "\n",
        "Once, we have found the optimum $\\boldsymbol{a}$, the prediction function of the SVR is given by\n",
        "\\begin{equation}\n",
        "y(\\mathbf x) = \\sum_{n=1}^N (a_n- \\widehat a _n)k (\\mathbf x, \\mathbf x _n) +b\n",
        "\\end{equation}\n",
        "where $b \\in \\mathbb{R}$ is the bias parameter.\n",
        "\n",
        "We can estimate $b$ by considering a data point for which $0 < a_n < C$, which must have $\\xi_n = 0$. Therefore this point must satisfy $\\epsilon + y_n - t_n = 0$.\n",
        "\\begin{equation}\n",
        "b = \\frac{1}{N_\\mathcal{M}} \\sum_{n \\in \\mathcal{M}} \\left( t_n - \\epsilon - \\sum_{m \\in \\mathcal{S}} (a_m- \\widehat a _m)k (\\mathbf x_n, \\mathbf x _m)\\right).\n",
        "\\end{equation}\n",
        "Analogous results can be obtained by considering a point for which $0 < \\widehat a_n < C$. $\\mathcal{S} \\subseteq \\{1, \\dots, N\\}$ denotes the set of support vectors and $\\mathcal{M} \\subseteq \\{1, \\dots, N\\}$ denotes the set of support vectors lying\n",
        "on the margin with $N_\\mathcal{M} = |\\mathcal{M}|$.\n",
        "\n",
        "> Below, implement a SVR for a simple regression problem by solving the dual problem above.\n",
        "> For optimization make use of `scipy` and its [Optimization Module](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#sequential-least-squares-programming-slsqp-algorithm-method-slsqp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "oJ5e1emUfDg8"
      },
      "outputs": [],
      "source": [
        "class RBFKernel:\n",
        "    def __init__(self, gamma=1):\n",
        "        \"\"\"Computes RBF kernel matrix between X_1 and X_2.\n",
        "\n",
        "        Args:\n",
        "            gamma (float): Hyperparameter of RBF kernel.\n",
        "        \"\"\"\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def __call__(self, X_1, X_2):\n",
        "        \"\"\"Computes the kernel matrix.\n",
        "\n",
        "        Args:\n",
        "            X_1 (array-like): Input samples in shape (N, D).\n",
        "            X_2 (array-like): Input samples in shape (N, D).\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Kernel matrix of shape shape (N, M)\n",
        "        \"\"\"\n",
        "        return np.exp(-self.gamma * np.sum((X_1[:, None] - X_2[None]) ** 2, axis=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "pfKW1BokfDg9"
      },
      "outputs": [],
      "source": [
        "class SVR:\n",
        "    def __init__(self, kernel_func, eps=0.2, C=1.0, random_state=42):\n",
        "        \"\"\"Implementation of a C-SVM for regression.\n",
        "        Args:\n",
        "            C (float): Regularization parameter. The strength of the regularization is inversely\n",
        "                proportional to C. Must be strictly positive. (default=1.0)\n",
        "            eps (float): ...\n",
        "            kernel_func (callable): Specifies the kernel type to be used in the algorithm.\n",
        "            random_state (int): Random state to ensure reproducibility when initializing  a values.\n",
        "        \"\"\"\n",
        "        self.C = C\n",
        "        self.eps = eps\n",
        "        self.kernel_func = kernel_func\n",
        "        self.random_state = random_state\n",
        "        np.random.seed(self.random_state)\n",
        "\n",
        "\n",
        "    def fit(self, X, t):\n",
        "        \"\"\"Fit the SVM model according to the given training data.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Training samples of shape (N, D).\n",
        "            t (array-like): Training targets of shape (N).\n",
        "\n",
        "        Returns:\n",
        "            self: The fitted SVM object.\n",
        "        \"\"\"\n",
        "        print(X.shape, t.shape)\n",
        "        K = self.kernel_func(X, X)\n",
        "        \n",
        "        # Optimization\n",
        "        # Step 1: Define the loss function and its gradient.\n",
        "        def loss(a):\n",
        "            # Compute loss for given a.\n",
        "            a_hat = a[:len(a)//2]\n",
        "            a_normal = a[len(a)//2:]\n",
        "            diff_a = (a_normal - a_hat)\n",
        "            return - 0.5 * diff_a.T @ K @ diff_a - self.eps * (a_normal + a_hat).T @ np.ones_like(diff_a) + diff_a.T @ t  # dual form\n",
        "\n",
        "        def jac(a):\n",
        "            # Compute gradient of loss function w.r.t. a. -> 2*N lang; gradient != jacobian\n",
        "            # concat grad w.r.t a_hat and a_normal\n",
        "            a_hat = a[:len(a)//2]\n",
        "            a_normal = a[len(a)//2:]\n",
        "          \n",
        "            deriv_t_a = -a_normal.T @ K  + 0.5 * a_hat.T @ K + 0.5 * a_hat @ K - self.eps + t\n",
        "            deriv_t_a_hat = a_normal.T @ K - a_hat @ K - self.eps - t\n",
        "            jac = np.concatenate([deriv_t_a_hat, deriv_t_a])\n",
        "            return jac\n",
        "\n",
        "        # Step 2: Define the Constraints.\n",
        "        # We need to write the contraints in matrix notation:\n",
        "        # - for inequalities: Ax <= b\n",
        "        # - for eqalities cx = d\n",
        "        # Note that x = a in our example.\n",
        "        # 'fun' in the constraints needs to be adapted such that\n",
        "        # 0 <= lambda a: ....\n",
        "\n",
        "        # Set up the constraints:\n",
        "        # Example: {'type': 'eq', 'fun': lambda a: a**2, 'jac': lambda a: 2*a}\n",
        "        constraints = [{'type': 'ineq', 'fun': lambda a: a[:len(a)//2], 'jac': lambda a: np.eye(len(a)//2)}]  # 0 <= a_hat\n",
        "        constraints += [{'type': 'ineq', 'fun': lambda a: a[len(a)//2:], 'jac': lambda a: np.eye(len(a)//2)}]  # 0 <= a_normal\n",
        "        \n",
        "        constraints += [{'type': 'ineq', 'fun': lambda a: self.C - a[:len(a)//2], 'jac': lambda a: -np.eye(len(a)//2)}]   # a_hat <= C\n",
        "        constraints += [{'type': 'ineq', 'fun': lambda a: self.C - a[len(a)//2:], 'jac': lambda a: -np.eye(len(a)//2)}]   # a_normal <= C\n",
        "\n",
        "        #constraints += [{'type': 'eq', 'fun': lambda a: np.sum(a[len(a)//2:] - a[:len(a)//2]), 'jac': lambda a: np.concatenate([-np.ones(len(a)//2), np.ones(len(a)//2)])}]  # sum(a_normal - a_hat) = 0\n",
        "\n",
        "\n",
        "        # Optimize the a vector.\n",
        "        a0 = np.random.rand(2 * len(X))  # initial guess\n",
        "        self.a_ = minimize(loss, a0, jac=jac, constraints=constraints, method='SLSQP').x\n",
        "        self.a_[np.isclose(self.a_, 0)] = 0  # zero out nearly zeros\n",
        "        self.a_[np.isclose(self.a_, self.C)] = self.C  # round the ones that are nearly C\n",
        "\n",
        "\n",
        "        # Determine indices of support vectors.\n",
        "        a_hat = self.a_[:len(self.a_)//2]\n",
        "        a_normal = self.a_[len(self.a_)//2:]\n",
        "\n",
        "        self.support_index_hat_ = np.where(a_hat == 0)[0]\n",
        "        self.support_index_normal_ = np.where(a_normal == 0)[0]\n",
        "        self.support_ = np.where((a_hat != 0) | (a_normal != 0))[0]\n",
        "        # Determine indices of support vectors that lie on the margin.\n",
        "        # support vectors are those with a_hat != 0 OR a_normal != 0\n",
        "        self.on_margin_ = np.where(self.a_ != 0)[0]\n",
        "        \n",
        "\n",
        "        # Determine bias parameter.\n",
        "        self.b = 1/len(self.on_margin_) * np.sum(t[self.on_margin_] - self.eps - np.sum(a_normal[self.support_index_normal_] * a_hat[self.support_index_hat_] @ K[self.support_][:, self.on_margin_], axis=0))\n",
        "\n",
        "\n",
        "        # Store support vectors including their targets and a.\n",
        "        self.support_vectors = X[self.support_]\n",
        "        self.support_targets = t[self.support_]\n",
        "        self.support_a = self.a_[self.support_]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Perform regression on samples in X.\n",
        "\n",
        "        Args:\n",
        "            X (array-like): Input samples whose targets are to be predicted.\n",
        "\n",
        "        Returns:\n",
        "            y (array-like): Predicted target of samples in X.\n",
        "        \"\"\"\n",
        "        a_hat = self.a_[:len(self.a_)//2]\n",
        "        a_normal = self.a_[len(self.a_)//2:]\n",
        "        K = self.kernel_func(X, self.support_vectors)\n",
        "        return (a_normal - a_hat).T @ K @ np.ones_like(self.a_) + self.b "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QywmWnXwfDg9"
      },
      "source": [
        "> Train the SVR on the given dataset and plot its support vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "qxu1w6IBfDg9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11,) (11,) [-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]\n",
            "(11, 1) (11,)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'constraints' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m svr \u001b[38;5;241m=\u001b[39m SVR(RBFKernel(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape, X)\n\u001b[0;32m----> 5\u001b[0m \u001b[43msvr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m svr\u001b[38;5;241m.\u001b[39mpredict(X)\n",
            "Cell \u001b[0;32mIn[53], line 72\u001b[0m, in \u001b[0;36mSVR.fit\u001b[0;34m(self, X, t)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Step 2: Define the Constraints.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# We need to write the contraints in matrix notation:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# - for inequalities: Ax <= b\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Optimize the a vector.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m a0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X))  \u001b[38;5;66;03m# initial guess\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_ \u001b[38;5;241m=\u001b[39m minimize(loss, a0, jac\u001b[38;5;241m=\u001b[39mjac, constraints\u001b[38;5;241m=\u001b[39m\u001b[43mconstraints\u001b[49m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSLSQP\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_[np\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_, \u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# zero out nearly zeros\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_[np\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC  \u001b[38;5;66;03m# round the ones that are nearly C\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'constraints' is not defined"
          ]
        }
      ],
      "source": [
        "X, y = get_simple()\n",
        "\n",
        "svr = SVR(RBFKernel(gamma=1), eps=0.2, C=1.0)\n",
        "print(X.shape, y.shape, X)\n",
        "svr.fit(X.reshape(-1,1), y)\n",
        "y_pred = svr.predict(X)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
